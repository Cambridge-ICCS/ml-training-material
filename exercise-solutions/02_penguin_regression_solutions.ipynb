{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Penguin regression with PyTorch\n",
    "\n",
    "<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"750\" />\n",
    "\n",
    "\n",
    "Artwork by @allison_horst\n",
    "\n",
    "In this exercise, we will again use the [``palmerpenguins``](https://github.com/mcnakhaee/palmerpenguins) data to continue our exploration of PyTorch.\n",
    "\n",
    "We will use the same dataset object as before, but this time we'll take a look at a regression problem: predicting the mass of a penguin given other physical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: look at the data\n",
    "In the following code block, we import the ``load_penguins`` function from the ``palmerpenguins`` package.\n",
    "\n",
    "- Load the penguin data as you did before.\n",
    "- This time, consider which features we might like to use to predict a penguin's mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palmerpenguins import load_penguins\n",
    "\n",
    "data = load_penguins()\n",
    "\n",
    "# Note: ``pd.DataFrame.describe`` is a useful function for giving an overiew\n",
    "# of what a ``pd.DataFrame`` contains.\n",
    "print(data.describe())\n",
    "\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now disuss the features we will use to classify the penguins' species, and populate the following list together:\n",
    "\n",
    "#### Let's use:\n",
    "\n",
    "- ``\"species\"``\n",
    "    - Perhaps the most relevant aspect from which to estimate mass.\n",
    "- ``\"sex\"``\n",
    "    - Biologically relevant.\n",
    "- ``\"bill_length_mm\"``\n",
    "    - Biologically relevant.\n",
    "- ``\"bill_depth_mm\"``\n",
    "    - Biologically relevant.\n",
    "- ``flipper_length_mm``\n",
    "    - Biologically relevant.\n",
    "\n",
    "#### Let's reject\n",
    "- ``\"island\"``\n",
    "    - While island could be predictive if dominated by a particular species it would be acting as a proxy and we have already included species as an input feature. \n",
    "- ``\"year\"``\n",
    "    - This feature could also be important: then behaviour of certain species may be changing in response to time-dependent environmental factors such as melting ice. It does however seem like the least biologically-relevant feature, and the most likely source of bias, so we reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: creating a ``torch.utils.data.Dataset``\n",
    "\n",
    "As before, we need to create PyTorch ``Dataset`` objects to supply data to our neural network.  \n",
    "Since we have already created and explored the ``PenguinDataset`` class there is nothing else to do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Obtaining training and validation datasets.\n",
    "\n",
    "- Instantiate the penguin dataloader.\n",
    "  - Make sure you supply the correct column titles for the features and the targets.\n",
    "  - Remember, the target is now mass, not the species!\n",
    "- Iterate over the dataset\n",
    "    - Hint:\n",
    "        ```python\n",
    "        for features, targets in dataset:\n",
    "            # print the features and targets here\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_workshop import PenguinDataset\n",
    "\n",
    "features = [\n",
    "    \"sex\",\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "]\n",
    "\n",
    "data_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"body_mass_g\"],\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "avg_mass_g = data.body_mass_g.mean()\n",
    "\n",
    "for _, (input_feats, target) in zip(range(20), data_set):\n",
    "    print(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Applying transforms to the data\n",
    "\n",
    "As in the previous exercise, the raw inputs and targets need transforming to ``torch.Tensor``s before they can be passed to a neural network.  \n",
    "We will again use ``torchvision.transforms.Compose`` to take a list of callable objects and apply them to the incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor, float32, eye\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "\n",
    "# Apply the transforms we need to the PenguinDataset to get out inputs\n",
    "# targets as Tensors.\n",
    "\n",
    "\n",
    "def get_input_transforms() -> Compose:\n",
    "    \"\"\"Return transforms which map from raw inputs to tensors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compose\n",
    "        A composition of transforms (callable functions) to map the the tuple\n",
    "        of input features (``Tuple[float, ...]``) to a ``torch.Tensor``.\n",
    "\n",
    "    \"\"\"\n",
    "    return Compose([lambda x: tensor(x, dtype=float32)])\n",
    "\n",
    "\n",
    "def get_target_tfms() -> Compose:\n",
    "    \"\"\"Return transforms which map from the raw targets to tensors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compose\n",
    "        A composition of transforms (callable functions) to map the the tuple\n",
    "        of target features (``Tuple[str]``) to a ``torch.Tensor``.\n",
    "\n",
    "    \"\"\"\n",
    "    return Compose([lambda x: tensor(x, dtype=float32)])\n",
    "\n",
    "\n",
    "train_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"body_mass_g\"],\n",
    "    train=True,\n",
    "    x_tfms=get_input_transforms(),\n",
    "    y_tfms=get_target_tfms(),\n",
    ")\n",
    "\n",
    "\n",
    "valid_set = PenguinDataset(\n",
    "    input_keys=features,\n",
    "    target_keys=[\"body_mass_g\"],\n",
    "    train=False,\n",
    "    x_tfms=get_input_transforms(),\n",
    "    y_tfms=get_target_tfms(),\n",
    ")\n",
    "\n",
    "\n",
    "for _, (input_feats, target) in zip(range(5), train_set):\n",
    "    print(input_feats, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Creating ``DataLoaders``â€”Again!\n",
    "\n",
    "As before, we wrap our ``Dataset``s in ``DataLoader`` before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch, targets in valid_loader:\n",
    "    print(batch.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Creating a neural network in PyTorch\n",
    "\n",
    "Previously we created our neural network from scratch, but doing this every time we need to solve a new problem is cumbersome.  \n",
    "Many projects working with the ICCS have codes where the numbers of layers, layer sizes, and other parts of the models are hard-coded from scratch every time!\n",
    "\n",
    "The result is ungainly, non-general, and heavily-duplicated code. Here, I am going to shamelessly punt my own Python repo, [``TorchTools``](https://github.com/jdenholm/TorchTools), which contains generalisations of many commonly-used PyTorch tools, to save save us some time.\n",
    "\n",
    "Here, we can use the ``FCNet`` model, whose documentation lives [here](https://jdenholm.github.io/TorchTools/models.html). This model is a fully-connected neural network with various options for dropout, batch normalisation, and easily-modifiable layers.\n",
    "\n",
    "#### A brief sidebar\n",
    "Note: the repo is pip-installable with\n",
    "```bash\n",
    "pip install git+https://github.com/jdenholm/TorchTools.git\n",
    "```\n",
    "but has already been installed for you in the requirements of this workshop package.\n",
    "\n",
    "It is useful to know you can install Python packages from GitHub using pip. To install specific versions you can use:\n",
    "```bash\n",
    "pip install git+https://github.com/jdenholm/TorchTools.git@v0.1.0\n",
    "```\n",
    "(The famous [segment anything model](https://github.com/facebookresearch/segment-anything) (SAM) published by Facebook Research was released in this way.)\n",
    "\n",
    "One might argue that this is a much better way of making one-off codes available, for example academic codes which might accompany papers, rather than using the global communal package index PyPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back to work: let's instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tools import FCNet\n",
    "\n",
    "model = FCNet(in_feats=4,\n",
    "              out_feats=1,\n",
    "              hidden_sizes=(16, 16),\n",
    "              input_bnorm=True,\n",
    "              input_dropout=0.1,\n",
    "              hidden_dropout=0.1,\n",
    "              hidden_bnorm=True,\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Selecting a loss function\n",
    "\n",
    "The previous loss function we chose was appopriate for classification, but _not_ regression.  \n",
    "Here we'll use the mean-squared-error loss, which is more appropriate for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "loss_func = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Selecting an optimiser\n",
    "\n",
    "``Adam`` is regarded as the king of optimisers: let's use it again.\n",
    "\n",
    "[Adam docs](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimiser and give it the model's parameters.\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimiser = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Writing basic training and validation loops\n",
    "\n",
    "\n",
    "As before, we will write the training loop together and you can then continue with the validation loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torch.nn import Module\n",
    "from torch import Tensor, no_grad\n",
    "from numpy import mean, sqrt\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: MSELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train ``model`` for once epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    train_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    optimiser : Adam\n",
    "        The optimiser.\n",
    "    loss_func : MSELoss\n",
    "        Mean squared error loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``model.train()`` is very important:\n",
    "        - it turns on the dropout layers.\n",
    "        - it tells the batch norm layers to use the incoming statistics, and let them contribute to their \"memory\".\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    metrics: Dict[str, float] = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for batch, targets in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        preds = model(batch)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        metrics[\"accuracy\"].append(batch_level_accuracy(preds, targets))\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    loss_func: MSELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Validate ``model`` for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The neural network.\n",
    "    valid_loader : DataLoader\n",
    "        Training dataloader.\n",
    "    loss_func : MSELoss\n",
    "        Mean squared error loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Metrics of interest.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The ``model.eval()`` is also very important:\n",
    "        - It turns off the dropout layers, which are likely to impair the validation performance and render it unrealistically poor.\n",
    "        - It tells the batchnorm layers to _not_ use the batch's statistics,\n",
    "        and to instead use the stats it has built up from the training set. The model should not \"remember\" anything from the validation set.\n",
    "    - We also protect this function with ``torch.no_grad()``, because having gradients enable while validating is a pointless waste of resourcesâ€”they are only needed for training.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics: Dict[str, float] = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for batch, targets in valid_loader:\n",
    "        preds = model(batch)\n",
    "\n",
    "        loss = loss_func(preds, targets)\n",
    "\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        metrics[\"accuracy\"].append(batch_level_accuracy(preds, targets))\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def batch_level_accuracy(preds: Tensor, targets: Tensor):\n",
    "    \"\"\"Compute the batch-level accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : Tensor\n",
    "        The model's predictions.\n",
    "    targets : Tensor\n",
    "        The corresponding labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The batch-level accuracy.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Here we use the MSE normalised by average mass as a measure of accuracy\n",
    "    \"\"\"\n",
    "    return ((preds - targets)**2).float().sqrt().mean()/avg_mass_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Training and extracting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - During each epoch the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turm them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict[str, float]]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from pandas import DataFrame\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "train_metrics, valid_metrics = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = perf_counter()\n",
    "\n",
    "    train_metrics.append(train_one_epoch(model, train_loader, optimiser, loss_func))\n",
    "\n",
    "    valid_metrics.append(validate_one_epoch(model, valid_loader, loss_func))\n",
    "\n",
    "    stop_time = perf_counter()\n",
    "\n",
    "    print(f\"Epoch {epoch} time: {stop_time - start_time:.3f} seconds.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "train_metrics = DataFrame(train_metrics)\n",
    "valid_metrics = DataFrame(valid_metrics)\n",
    "\n",
    "metrics = train_metrics.join(valid_metrics, lsuffix=\"_train\", rsuffix=\"_valid\")\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 11: Plotting metrics\n",
    "\n",
    "- Use Matplotlib to plot the training and validation metrics as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace\n",
    "\n",
    "\n",
    "quantities = [\"loss\", \"accuracy\"]\n",
    "splits = [\"train\", \"valid\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for axis, quant in zip(axes.ravel(), quantities):\n",
    "    for split in splits:\n",
    "        key = f\"{quant}_{split}\"\n",
    "        axis.plot(\n",
    "            linspace(1, epochs, epochs),\n",
    "            metrics[key],\n",
    "            \"-o\",\n",
    "            label=split.capitalize(),\n",
    "        )\n",
    "    axis.set_ylabel(quant.capitalize(), fontsize=15)\n",
    "\n",
    "for axis in axes.ravel():\n",
    "    axis.legend(fontsize=15)\n",
    "    axis.set_ylim(bottom=0.0, top=1.0)\n",
    "    axis.set_xlim(left=1, right=epochs)\n",
    "    axis.set_xlabel(\"Epoch\", fontsize=15)\n",
    "\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
