---
title: "Introduction to Machine Learning with PyTorch"
subtitle: "ICCS Summer school 2023"
format:
  revealjs:
    embed-resources: true
    slide-number: true
    chalkboard: false
    preview-links: auto
    history: false
    logo: https://iccs.cam.ac.uk/sites/iccs.cam.ac.uk/files/logo2_2.png
    theme: dark
    render-on-save: true
---

# Stochastic gradient descent (SGD) ...

## ... in the context of fitting a function

- Generally speaking, most neural networks are fit/trained using SGD (or some variant of it).

- To understand the basics of how one might fit a function with SGD, let's do it with a straight line: $$y=mx+c$$


## Fitting a straight line I {.smaller}

- **Question**---when we a differentiate a function, what do we get?

- Consider:

$$y = mx + c$$

$$\frac{dy}{dx} = m$$

- $m$ is certainly $y$'s slope, but is there a (perhaps) more fundamental way to view a derivative?

## Fitting a straight line II {.smaller}

- **Answer**---a function's derivative gives a _vector_ which points in the direction of _steepest ascent_.

:::: {.columns}

::: {.column width="50%"}

- Consider

$$y = x$$

$$\frac{dy}{dx} = 1$$

:::

::: {.column width="50%"}
- Plot
:::

::::

- The derivative points in the _positive_ direction---the direction of steepest ascent.

- What is the direction of _steepest descent?_
$$-\frac{dy}{dx}$$



## What about neural networks?

## Neural Net Basics
It's matrix multiplication, innit.

## Classification and Regression


# Python and PyTorch

## Python projects

## PyTorch

# More theory

# Applications


# Further information

## Slides

## Contact

## Resources
