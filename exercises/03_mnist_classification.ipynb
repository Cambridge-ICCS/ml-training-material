{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying handwritten digits with a CNN\n",
    "\n",
    "\n",
    "In the previous exercies, we worked with fully-connected neural networks, which are good at handling tabular data, where the inputs and targets are easily presented as vectors.\n",
    "\n",
    "However, in the case of images, or image-like objects, such models are less efficient for reasons we have discussed in the slides. When inputs are images, or image-like data, a more natural choice of model is a convolution neural network—in particularly, a model which uses 2D convolutional layers.\n",
    "\n",
    "Before we start worrying about choosing models, let's first acquaint ourselves with the MNIST data.\n",
    "\n",
    "The first step is to select a directory for the data to live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_root = \"mnist-data/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we all set a path this way it will help to maintain consistency throughout this exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Access the MNIST dataset.\n",
    "\n",
    "PyTorch has a (sort of) sister Python library for dealing with images: [``Torchvision``](https://pytorch.org/vision/stable/index.html) (take a look at the website for a few minutes).\n",
    "\n",
    "In the previous exercies, we used a custom ``Dataset`` object created specifically for this event, but with ``Torchvision`` comes a bunch of easy-to-use datasets, one of which is MNIST.\n",
    "\n",
    "- Look at the arguments of the MNIST datset: what options do we have?\n",
    "- Instantiate the (training) dataset.\n",
    "- Iterate over it: how are the inputs and targets presented to us?\n",
    "- Plot some images, and set their targets as the title, to make sure the data make sense.\n",
    "\n",
    "Note: this section might prove a bit tricky for many people, so I will go through it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_set = MNIST(root=mnist_root, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Task 2\n",
    "\n",
    "As before, we have a dataset which gives us inputs and targets, but we still need to convert the data to ``torch.Tensors``\n",
    "\n",
    "#### Part (a) — raw data to ``Tensor``s\n",
    "\n",
    "Let's do this basic steps first: supply transforms to map between the raw data and ``torch.Tensors``.\n",
    "\n",
    "\n",
    "\n",
    "#### Part (b) — fun with data augmentation\n",
    "\n",
    "Additionally, with image data there are some other consideration we might make:\n",
    "- Are CNNs rotationally invariant?\n",
    "  - If we want out model still to work on images which are not of a regular orientation, we must use random rotations as a form of augmentation.\n",
    "- If we train a model on purely black-and-white images, how will it fare on more colourful data?\n",
    "- Go to ``Torchvision``'s [transforms](https://pytorch.org/vision/stable/transforms.html) and look at the the available forms of images augmentation.\n",
    "  - Take a few minutes to pick ones you think might be relevant.\n",
    "  - Let's discuss and choose some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor, Compose\n",
    "from torch import Tensor, tensor\n",
    "\n",
    "def get_img_tfms(training: bool) -> Compose:\n",
    "    \"\"\"Return a composition of image transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training : bool\n",
    "        Are we training? If ``False``, we are validating.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Compose\n",
    "        A composition of image transforms.\n",
    "\n",
    "    \"\"\"\n",
    "    tfm_list: List[Callable] = [ToTensor(), lambda x: x.repeat(3, 1, 1)]\n",
    "\n",
    "    if training is True:\n",
    "        # tfm_list.append(???)\n",
    "        pass\n",
    "\n",
    "    return Compose(tfm_list)\n",
    "\n",
    "\n",
    "def target_as_tensor(target_idx: int) -> Tensor:\n",
    "    \"\"\"Express the target as a ``torch.Tensor``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_idx : int\n",
    "        The ground truth (i.e. the number shown in the image).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        The target as a one-hot-encoded vector.\n",
    "\n",
    "    \"\"\"\n",
    "    return tensor(target_idx)\n",
    "\n",
    "\n",
    "train_set = MNIST(\n",
    "    mnist_root,\n",
    "    transform=get_img_tfms(training=True),\n",
    "    target_transform=target_as_tensor,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "\n",
    "valid_set = MNIST(\n",
    "    mnist_root,\n",
    "    transform=get_img_tfms(training=False),\n",
    "    target_transform=target_as_tensor,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "figure, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for idx, axis in enumerate(axes.ravel()):\n",
    "\n",
    "    image, target = train_set[idx]\n",
    "\n",
    "    axis.imshow(image.permute(1, 2, 0), cmap=\"inferno\")\n",
    "    axis.set_title(target)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: ``Dataset`` $\\to$ ``DataLoader``\n",
    "\n",
    "As before, wrap the ``Dataset``'s in ``DataLoader``s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Choose a model architecture\n",
    "\n",
    "Torchvision provides a collection of models, [here](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Since we are all on laptops, some of which don't have CUDA-enabled GPUs, I suggest we choose a modest neural network than won't melt any of our hardware. One such network, designed with mobile phones in mind, is [``MOBILENET``](https://pytorch.org/vision/stable/models/mobilenetv3.html).\n",
    "\n",
    "#### Task 4 (a): instantiate the small one, and print it out.\n",
    "\n",
    "Note:\n",
    "- Torchvision's models can optionally be endowed with pretrained weights (from corresponding models pretrain on the ImageNet dataset).\n",
    "- We can (optionally) supply these weights.\n",
    "  - Using weights from a model, trained on one problem, as an intitial condition in another problem, is called transfer learning.\n",
    "  - Why do you think this might be advantageous, even in disparate problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import mobilenet_v3_small\n",
    "from torchvision.models import MobileNet_V3_Small_Weights\n",
    "\n",
    "model = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 (b): Uh oh, we've a problem.\n",
    "\n",
    "- Look at the final linear layer.\n",
    "  - How many output classes are there?\n",
    "  - How many do we need?\n",
    "- We need to \"overload\" the final layer to produce the correct number of output features for our problem. Fortunately this is easy.\n",
    "- Uncomment the code below, choose the correct number of output features, and print the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "# model.classifier[3] = Linear(model.classifier[3].in_features, ???)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Set up the remaining PyTorch bits and bobs\n",
    "\n",
    "- We need to choose a loss function appropriate for classification.\n",
    "  - Can you remember what we chose previously?\n",
    "- We need an optimiser, too.\n",
    "  - Remember our friend, Adam?\n",
    "- Instantiate the model and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Set the device\n",
    "\n",
    "We could have done this when we created the model, but it's fine here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available\n",
    "\n",
    "DEVICE = \"cuda\" if is_available() else \"cpu\"\n",
    "\n",
    "# Note: here the return/assignment to ``_`` is just to supress the print.\n",
    "# The model is moved onto the correct device in-place.\n",
    "_ = model.to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Writing our training and validation loops\n",
    "\n",
    "As before, we need to write our training and validation loops.\n",
    "\n",
    "- Complete the training loop\n",
    "- Complete the validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from torch import no_grad\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: BCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        A neural network.\n",
    "    train_loader : DataLoader\n",
    "        The ``DataLoader`` for the training set.\n",
    "    optimiser : Adam\n",
    "        The optimiser to update the model's parameters with.\n",
    "    loss_func : BCELoss\n",
    "        Binary-cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "\n",
    "@no_grad()\n",
    "def validate_one_epoch(\n",
    "    model: Module,\n",
    "    valid_loader: DataLoader,\n",
    "    optimiser: Adam,\n",
    "    loss_func: BCELoss,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Train the model for a single epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        A neural network.\n",
    "    valid_loader : DataLoader\n",
    "        The ``DataLoader`` for the training set.\n",
    "    loss_func : BCELoss\n",
    "        Binary-cross-entropy loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        A dictionary of metrics.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Training and extracting metrics\n",
    "\n",
    "- Now we can train our model for a specified number of epochs.\n",
    "  - One epoch is where the model \"sees\" each training item once.\n",
    "- Append the training and validation metrics to a list.\n",
    "- Turm them into a ``pandas.DataFrame``\n",
    "  - Note: You can turn a ``List[Dict[str, float]]``, say ``my_list`` into a ``DataFrame`` with ``DataFrame(my_list)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "for _ in range(epochs):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Plotting metrics\n",
    "\n",
    " - Let plots the training and validation metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10: Visualising some predictions\n",
    "\n",
    "Let's pick some random validation items, predict on them, and visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_inds = [0, 666, 1024, 2048, 5555]\n",
    "\n",
    "# Stick the model on the cpu\n",
    "_ = model.to(\"cpu\")\n",
    "_ = model.eval()\n",
    "\n",
    "# figure, axes = plt.subplots(1, len(valid_inds), figsize=(2 * len(valid_inds), 2))\n",
    "\n",
    "# for idx, axis in zip(valid_inds, axes.ravel()):\n",
    "\n",
    "#     img_tensor, _ = valid_set[idx]\n",
    "\n",
    "#     print(img_tensor.shape)\n",
    "\n",
    "#     with no_grad():\n",
    "#         pred = model(img_tensor.unsqueeze(0)).softmax(dim=1).argmax(dim=1).item()\n",
    "\n",
    "\n",
    "#     axis.imshow(img_tensor.permute(1, 2, 0))\n",
    "#     axis.set_title(pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguins",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
